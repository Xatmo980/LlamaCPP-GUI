# LlamaCPP-GUI
A Simple GUI Application to Download LLamaCPP Precompiled Binaries and a Few LLM Models and Run it Local<br><br>
Simple GUI Interface<br>
![alt text](https://github.com//Xatmo980/LlamaCPP-GUI/blob/main/GUI.jpg?raw=true)<br><br>
Commandline Chat<br>
![alt text](https://github.com//Xatmo980/LlamaCPP-GUI/blob/main/CLIChat.jpg?raw=true)<br><br>
Web Browser Chat<br>
![alt text](https://github.com//Xatmo980/LlamaCPP-GUI/blob/main/Web.jpg?raw=true)<br><br>
(----------------------------------------------------------------------------------------)<br>
(-----------------------------------------Models--------------------------------------)<br><br>
It Downloads and Runs the Following Models<br><br>
DeepSeek-R1-Distill-Llama-8B<br>
DeepSeek-R1-Distill-Qwen-7B<br>
Dolphin3.0-Llama3.1-8B<br>
Qwen2.5-7B-Instruct-1M<br>
gemma-2-9b-it<br>
WizardLM-2-7B<br>
(----------------------------------------------------------------------------------------)<br>
(-------------------------------------troubleshooting---------------------------------)<br><br>
If the model you downloaded does not load, try lowering the GPU Layers.<br>
I have about an 8GB VRAM GPU soo ive noticed these values work on my gpu for the models<br><br>
DeepSeek R1 Qwuen 7b  (25-26)Layers<br>
Dolphin3.0-Lllama3.1 8B (30-33)Layers<br>
Qwen2.5-7B-Instruct-1M (26)Layers<br>
WizardLM-2-7B (26-29)Layers<br>
