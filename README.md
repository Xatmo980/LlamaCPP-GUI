# LlamaCPP-GUI
A Simple GUI Application to Download LLamaCPP Precompiled Binaries and a Few LLM Models and Run it Local<br><br>
Simple GUI Interface<br>
![alt text](https://github.com//Xatmo980/LlamaCPP-GUI/blob/main/GUI.jpg?raw=true)<br><br>
Commandline Chat<br>
![alt text](https://github.com//Xatmo980/LlamaCPP-GUI/blob/main/CLIChat.jpg?raw=true)<br><br>
Web Browser Chat<br>
![alt text](https://github.com//Xatmo980/LlamaCPP-GUI/blob/main/Web.jpg?raw=true)<br><br>
(----------------------------------------------------------------------------------------)<br>
(-------------------------------------troubleshooting---------------------------------)<br><br>
If the model you downloaded does not load, try lowering the GPU Layers.<br>
I have about 8GB VRAM GPU soo ive noticed thease values work on my gpu for two of the models<br><br>
DeepSeek R1 Qwuen 7b  (25-29)layers<br>
Dolphin3.0-Lllama3.1 8B (30-33)layers
